{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "# easy way to shuffle the dataset\n",
    "train = train.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train[\"label\"])\n",
    "x_train = np.array(train.drop(labels = [\"label\"],axis = 1))\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4132 4684 4177 4351 4072 3795 4137 4401 4063 4188]\n",
      "3795\n"
     ]
    }
   ],
   "source": [
    "count = np.zeros(10).astype('int64')\n",
    "\n",
    "for i in y_train:\n",
    "    count[i] += 1\n",
    "\n",
    "min_val = np.min(count)\n",
    "print(count)\n",
    "print(min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33592, 33597, 33607, 33614, 33617, 33639, 33642, 33646, 33656, 33663, 33665, 33673, 33676, 33701, 33702, 33737, 33745, 33749, 33751, 33752, 33774, 33792, 33820, 33832, 33833, 33851, 33869, 33870, 33871, 33890, 33924, 33929, 33939, 33942, 33943, 33954, 33979, 33992, 33997, 33999, 34002, 34005, 34012, 34020, 34021, 34029, 34039, 34049, 34061, 34071, 34077, 34079, 34085, 34110, 34123, 34132, 34151, 34152, 34177, 34179, 34180, 34185, 34202, 34210, 34224, 34228, 34234, 34247, 34266, 34283, 34297, 34304, 34316, 34317, 34329, 34336, 34339, 34361, 34380, 34389, 34396, 34403, 34407, 34408, 34409, 34430, 34437, 34447, 34471, 34477, 34481, 34484, 34498, 34501, 34506, 34524, 34533, 34551, 34557, 34567, 34584, 34600, 34601, 34613, 34623, 34643, 34687, 34691, 34694, 34702, 34703, 34708, 34715, 34721, 34731, 34734, 34738, 34752, 34776, 34778, 34818, 34824, 34827, 34842, 34860, 34871, 34880, 34881, 34894, 34898, 34906, 34912, 34915, 34936, 34941, 34942, 34953, 34978, 34991, 34993, 35004, 35008, 35020, 35025, 35028, 35037, 35044, 35048, 35079, 35098, 35100, 35110, 35113, 35114, 35119, 35125, 35132, 35138, 35139, 35159, 35171, 35174, 35176, 35184, 35201, 35202, 35234, 35235, 35239, 35256, 35263, 35269, 35272, 35293, 35314, 35317, 35318, 35324, 35347, 35352, 35355, 35364, 35368, 35386, 35408, 35420, 35429, 35433, 35452, 35462, 35469, 35502, 35519, 35524, 35528, 35530, 35540, 35562, 35578, 35582, 35587, 35591, 35596, 35613, 35625, 35630, 35635, 35636, 35640, 35642, 35652, 35668, 35670, 35673, 35674, 35677, 35692, 35696, 35702, 35707, 35710, 35716, 35734, 35769, 35770, 35779, 35781, 35792, 35793, 35795, 35810, 35830, 35844, 35851, 35859, 35864, 35869, 35883, 35891, 35893, 35913, 35922, 35925, 35936, 35949, 35952, 35954, 35957, 35979, 35980, 35981, 35983, 35993, 36002, 36003, 36006, 36014, 36019, 36021, 36029, 36049, 36051, 36060, 36064, 36065, 36079, 36090, 36093, 36099, 36105, 36107, 36122, 36130, 36139, 36160, 36168, 36177, 36202, 36206, 36219, 36224, 36232, 36241, 36245, 36252, 36254, 36256, 36257, 36261, 36269, 36270, 36279, 36283, 36284, 36291, 36293, 36294, 36300, 36305, 36310, 36320, 36324, 36335, 36338, 36346, 36359, 36365, 36368, 36371, 36373, 36386, 36391, 36396, 36410, 36412, 36418, 36427, 36429, 36435, 36436, 36447, 36448, 36451, 36453, 36457, 36462, 36469, 36479, 36488, 36489, 36494, 36498, 36499, 36501, 36502, 36503, 36510, 36514, 36515, 36518, 36519, 36525, 36527, 36532, 36536, 36538, 36539, 36542, 36545, 36547, 36548, 36551, 36554, 36556, 36558, 36562, 36563, 36564, 36565, 36568, 36581, 36583, 36586, 36591, 36594, 36595, 36600, 36601, 36605, 36607, 36608, 36611, 36612, 36617, 36623, 36625, 36627, 36628, 36631, 36632, 36633, 36636, 36639, 36645, 36653, 36656, 36659, 36662, 36667, 36669, 36670, 36680, 36689, 36693, 36694, 36697, 36699, 36707, 36709, 36716, 36719, 36720, 36721, 36722, 36728, 36732, 36735, 36736, 36740, 36741, 36743, 36745, 36747, 36748, 36751, 36761, 36767, 36768, 36771, 36775, 36776, 36779, 36780, 36783, 36785, 36789, 36790, 36791, 36794, 36797, 36798, 36799, 36801, 36807, 36811, 36813, 36815, 36816, 36818, 36819, 36824, 36828, 36836, 36838, 36840, 36841, 36847, 36848, 36852, 36853, 36855, 36859, 36862, 36864, 36865, 36867, 36870, 36871, 36874, 36875, 36878, 36879, 36883, 36884, 36885, 36890, 36894, 36895, 36899, 36904, 36906, 36910, 36911, 36917, 36919, 36925, 36926, 36927, 36934, 36935, 36936, 36937, 36938, 36946, 36948, 36950, 36952, 36955, 36959, 36962, 36963, 36966, 36967, 36968, 36980, 36983, 36990, 36995, 37000, 37002, 37004, 37006, 37008, 37009, 37011, 37012, 37013, 37017, 37019, 37022, 37024, 37032, 37033, 37036, 37040, 37042, 37047, 37049, 37053, 37055, 37057, 37058, 37062, 37065, 37070, 37072, 37074, 37078, 37079, 37080, 37083, 37095, 37096, 37098, 37102, 37105, 37118, 37122, 37123, 37129, 37133, 37137, 37138, 37151, 37155, 37156, 37157, 37163, 37164, 37171, 37174, 37181, 37188, 37189, 37190, 37191, 37199, 37202, 37203, 37204, 37205, 37209, 37210, 37212, 37215, 37217, 37218, 37221, 37222, 37223, 37227, 37232, 37235, 37241, 37243, 37245, 37246, 37247, 37253, 37257, 37259, 37263, 37271, 37275, 37276, 37280, 37285, 37287, 37292, 37293, 37294, 37295, 37304, 37309, 37310, 37311, 37312, 37318, 37319, 37320, 37327, 37328, 37331, 37333, 37338, 37339, 37344, 37359, 37360, 37362, 37366, 37372, 37375, 37379, 37383, 37385, 37386, 37387, 37390, 37395, 37398, 37400, 37401, 37403, 37405, 37407, 37408, 37410, 37411, 37416, 37417, 37423, 37429, 37433, 37438, 37443, 37448, 37449, 37452, 37455, 37457, 37465, 37472, 37485, 37491, 37495, 37496, 37498, 37500, 37501, 37508, 37512, 37513, 37514, 37515, 37519, 37522, 37530, 37531, 37535, 37542, 37543, 37544, 37548, 37552, 37553, 37555, 37556, 37558, 37568, 37571, 37574, 37578, 37582, 37587, 37588, 37590, 37594, 37597, 37598, 37603, 37605, 37609, 37611, 37612, 37619, 37631, 37637, 37639, 37641, 37642, 37645, 37646, 37654, 37659, 37665, 37669, 37673, 37677, 37679, 37681, 37682, 37683, 37684, 37687, 37693, 37694, 37698, 37701, 37702, 37705, 37718, 37725, 37730, 37732, 37741, 37744, 37745, 37750, 37751, 37753, 37756, 37760, 37761, 37765, 37767, 37768, 37769, 37771, 37773, 37775, 37777, 37779, 37780, 37781, 37788, 37799, 37801, 37805, 37815, 37821, 37823, 37829, 37832, 37836, 37838, 37840, 37845, 37846, 37855, 37858, 37859, 37864, 37867, 37874, 37877, 37878, 37885, 37886, 37889, 37890, 37891, 37892, 37898, 37900, 37901, 37903, 37905, 37908, 37910, 37914, 37923, 37926, 37927, 37931, 37940, 37946, 37951, 37954, 37957, 37963, 37964, 37965, 37968, 37975, 37976, 37977, 37982, 37989, 37991, 37992, 37993, 37995, 37997, 37998, 38001, 38007, 38010, 38020, 38026, 38029, 38030, 38032, 38033, 38035, 38037, 38040, 38042, 38048, 38051, 38053, 38054, 38055, 38056, 38058, 38059, 38062, 38063, 38065, 38066, 38068, 38069, 38072, 38074, 38075, 38077, 38081, 38083, 38086, 38097, 38100, 38101, 38108, 38113, 38114, 38115, 38119, 38120, 38123, 38124, 38126, 38129, 38131, 38134, 38135, 38136, 38139, 38140, 38142, 38146, 38159, 38160, 38164, 38165, 38166, 38169, 38171, 38172, 38176, 38179, 38180, 38181, 38183, 38185, 38187, 38188, 38190, 38194, 38197, 38198, 38199, 38201, 38202, 38204, 38208, 38209, 38213, 38214, 38215, 38218, 38227, 38229, 38232, 38233, 38238, 38240, 38244, 38247, 38248, 38250, 38252, 38257, 38258, 38260, 38264, 38266, 38277, 38280, 38281, 38288, 38290, 38292, 38295, 38298, 38300, 38301, 38303, 38305, 38307, 38308, 38310, 38311, 38315, 38317, 38323, 38325, 38328, 38330, 38332, 38334, 38335, 38337, 38339, 38342, 38343, 38344, 38345, 38348, 38349, 38357, 38361, 38362, 38363, 38364, 38374, 38376, 38378, 38379, 38380, 38382, 38383, 38385, 38386, 38388, 38392, 38394, 38395, 38396, 38397, 38398, 38404, 38405, 38407, 38409, 38410, 38411, 38413, 38414, 38415, 38418, 38419, 38421, 38423, 38425, 38427, 38428, 38433, 38434, 38437, 38438, 38439, 38444, 38445, 38447, 38448, 38449, 38450, 38451, 38452, 38453, 38454, 38455, 38456, 38458, 38461, 38464, 38467, 38471, 38473, 38476, 38478, 38479, 38482, 38483, 38484, 38486, 38487, 38488, 38489, 38492, 38493, 38494, 38495, 38496, 38497, 38498, 38499, 38503, 38504, 38505, 38507, 38508, 38510, 38511, 38512, 38515, 38518, 38519, 38520, 38522, 38524, 38525, 38527, 38528, 38529, 38530, 38531, 38532, 38533, 38534, 38536, 38537, 38538, 38539, 38540, 38542, 38543, 38544, 38547, 38548, 38549, 38550, 38551, 38552, 38554, 38556, 38559, 38560, 38561, 38562, 38563, 38564, 38565, 38567, 38568, 38569, 38571, 38573, 38574, 38575, 38576, 38577, 38578, 38579, 38580, 38581, 38582, 38583, 38584, 38585, 38587, 38588, 38590, 38591, 38594, 38596, 38597, 38598, 38599, 38600, 38602, 38603, 38604, 38606, 38607, 38608, 38609, 38611, 38612, 38613, 38614, 38616, 38618, 38619, 38620, 38621, 38622, 38623, 38624, 38625, 38626, 38627, 38628, 38629, 38630, 38632, 38633, 38634, 38637, 38638, 38640, 38641, 38642, 38643, 38646, 38648, 38649, 38650, 38652, 38653, 38655, 38657, 38659, 38660, 38661, 38663, 38665, 38666, 38668, 38669, 38673, 38674, 38675, 38676, 38677, 38678, 38679, 38680, 38681, 38684, 38687, 38688, 38689, 38691, 38693, 38695, 38696, 38698, 38699, 38703, 38705, 38707, 38709, 38711, 38712, 38713, 38714, 38715, 38716, 38718, 38720, 38721, 38725, 38727, 38732, 38733, 38734, 38735, 38737, 38738, 38739, 38740, 38741, 38744, 38745, 38746, 38751, 38752, 38753, 38756, 38757, 38758, 38759, 38761, 38763, 38766, 38768, 38770, 38772, 38775, 38777, 38778, 38779, 38780, 38781, 38786, 38787, 38789, 38791, 38793, 38794, 38795, 38797, 38798, 38799, 38800, 38801, 38802, 38803, 38804, 38805, 38807, 38808, 38809, 38810, 38811, 38813, 38814, 38815, 38816, 38817, 38819, 38820, 38821, 38822, 38823, 38824, 38827, 38829, 38831, 38833, 38834, 38836, 38837, 38838, 38839, 38840, 38841, 38842, 38843, 38844, 38845, 38847, 38848, 38850, 38852, 38853, 38855, 38856, 38859, 38861, 38862, 38865, 38866, 38867, 38868, 38869, 38872, 38873, 38874, 38875, 38878, 38879, 38882, 38885, 38886, 38887, 38888, 38889, 38890, 38891, 38892, 38893, 38895, 38897, 38898, 38899, 38900, 38902, 38903, 38904, 38905, 38906, 38907, 38909, 38911, 38912, 38913, 38914, 38915, 38916, 38917, 38918, 38920, 38921, 38922, 38923, 38926, 38927, 38928, 38929, 38930, 38931, 38933, 38934, 38935, 38936, 38937, 38938, 38941, 38942, 38944, 38945, 38946, 38947, 38948, 38949, 38950, 38951, 38952, 38953, 38954, 38955, 38956, 38957, 38958, 38959, 38960, 38961, 38962, 38963, 38964, 38965, 38969, 38970, 38971, 38972, 38973, 38974, 38975, 38976, 38977, 38978, 38980, 38981, 38983, 38984, 38986, 38987, 38989, 38990, 38992, 38993, 38994, 38996, 38997, 38998, 38999, 39002, 39005, 39006, 39007, 39009, 39010, 39013, 39014, 39017, 39018, 39020, 39022, 39023, 39024, 39025, 39026, 39027, 39028, 39030, 39032, 39033, 39034, 39035, 39036, 39037, 39039, 39040, 39041, 39042, 39043, 39045, 39046, 39048, 39049, 39050, 39051, 39052, 39053, 39054, 39055, 39056, 39057, 39059, 39060, 39062, 39063, 39066, 39069, 39071, 39072, 39073, 39074, 39075, 39076, 39077, 39078, 39079, 39080, 39081, 39082, 39084, 39087, 39088, 39089, 39090, 39091, 39093, 39094, 39095, 39097, 39098, 39100, 39101, 39102, 39103, 39104, 39107, 39108, 39110, 39111, 39112, 39113, 39114, 39117, 39118, 39119, 39120, 39121, 39123, 39124, 39125, 39127, 39129, 39131, 39132, 39135, 39136, 39138, 39139, 39140, 39141, 39142, 39143, 39145, 39146, 39147, 39148, 39151, 39152, 39154, 39155, 39156, 39157, 39158, 39159, 39160, 39161, 39163, 39164, 39165, 39166, 39167, 39169, 39170, 39171, 39172, 39174, 39176, 39178, 39179, 39180, 39181, 39183, 39184, 39185, 39188, 39190, 39191, 39193, 39194, 39195, 39196, 39197, 39198, 39199, 39200, 39201, 39203, 39206, 39207, 39209, 39211, 39212, 39213, 39214, 39215, 39217, 39218, 39219, 39220, 39222, 39224, 39229, 39230, 39231, 39234, 39235, 39237, 39240, 39241, 39242, 39244, 39245, 39246, 39247, 39248, 39249, 39250, 39251, 39253, 39254, 39255, 39256, 39257, 39259, 39261, 39262, 39263, 39264, 39265, 39266, 39267, 39268, 39269, 39270, 39271, 39272, 39273, 39274, 39275, 39276, 39277, 39278, 39279, 39280, 39281, 39282, 39283, 39284, 39285, 39286, 39287, 39288, 39289, 39290, 39292, 39293, 39294, 39295, 39297, 39298, 39299, 39300, 39302, 39303, 39304, 39305, 39306, 39307, 39308, 39309, 39310, 39311, 39312, 39313, 39315, 39316, 39317, 39318, 39319, 39320, 39321, 39322, 39323, 39324, 39325, 39326, 39327, 39328, 39329, 39330, 39331, 39332, 39333, 39334, 39335, 39336, 39337, 39338, 39339, 39340, 39341, 39343, 39344, 39346, 39348, 39349, 39350, 39351, 39352, 39353, 39354, 39355, 39356, 39357, 39358, 39359, 39360, 39361, 39362, 39363, 39364, 39365, 39366, 39367, 39368, 39369, 39371, 39372, 39373, 39374, 39375, 39376, 39377, 39378, 39379, 39380, 39381, 39382, 39383, 39384, 39385, 39386, 39387, 39388, 39389, 39390, 39391, 39393, 39394, 39395, 39396, 39398, 39399, 39400, 39401, 39402, 39403, 39404, 39406, 39407, 39408, 39410, 39411, 39413, 39414, 39415, 39416, 39417, 39418, 39420, 39421, 39422, 39423, 39424, 39425, 39426, 39427, 39428, 39429, 39430, 39431, 39432, 39433, 39434, 39435, 39437, 39438, 39439, 39440, 39441, 39443, 39444, 39445, 39446, 39447, 39448, 39449, 39451, 39452, 39453, 39455, 39456, 39457, 39458, 39459, 39460, 39462, 39463, 39464, 39465, 39466, 39467, 39468, 39469, 39470, 39471, 39472, 39473, 39474, 39475, 39476, 39477, 39478, 39479, 39480, 39481, 39482, 39484, 39485, 39486, 39487, 39488, 39489, 39490, 39491, 39492, 39493, 39494, 39495, 39496, 39497, 39498, 39500, 39501, 39502, 39503, 39504, 39505, 39506, 39507, 39508, 39509, 39510, 39511, 39512, 39513, 39514, 39515, 39516, 39517, 39518, 39519, 39520, 39523, 39524, 39525, 39526, 39527, 39528, 39529, 39530, 39531, 39532, 39533, 39534, 39535, 39536, 39537, 39538, 39539, 39540, 39542, 39543, 39544, 39545, 39546, 39547, 39548, 39549, 39551, 39552, 39553, 39554, 39556, 39557, 39558, 39559, 39560, 39561, 39562, 39563, 39564, 39565, 39566, 39567, 39568, 39569, 39570, 39571, 39572, 39575, 39576, 39577, 39578, 39579, 39580, 39581, 39582, 39583, 39584, 39585, 39586, 39587, 39588, 39590, 39591, 39592, 39593, 39594, 39595, 39596, 39597, 39598, 39600, 39601, 39602, 39603, 39604, 39605, 39606, 39607, 39608, 39609, 39610, 39611, 39612, 39613, 39614, 39615, 39616, 39617, 39618, 39619, 39620, 39621, 39623, 39624, 39625, 39626, 39627, 39628, 39629, 39630, 39631, 39632, 39633, 39634, 39635, 39636, 39637, 39638, 39639, 39640, 39641, 39642, 39643, 39644, 39645, 39646, 39647, 39648, 39649, 39650, 39651, 39652, 39653, 39654, 39655, 39656, 39657, 39658, 39659, 39660, 39661, 39662, 39663, 39664, 39665, 39666, 39667, 39668, 39669, 39670, 39671, 39672, 39673, 39674, 39676, 39677, 39678, 39679, 39680, 39681, 39682, 39683, 39684, 39685, 39686, 39687, 39688, 39689, 39690, 39692, 39693, 39695, 39697, 39698, 39699, 39700, 39701, 39702, 39703, 39704, 39705, 39706, 39707, 39708, 39709, 39710, 39711, 39713, 39714, 39715, 39717, 39718, 39719, 39720, 39721, 39722, 39723, 39724, 39725, 39726, 39727, 39728, 39729, 39730, 39732, 39733, 39734, 39736, 39737, 39738, 39739, 39741, 39742, 39743, 39744, 39745, 39746, 39747, 39748, 39749, 39750, 39751, 39752, 39753, 39754, 39755, 39756, 39757, 39758, 39759, 39760, 39761, 39762, 39763, 39764, 39765, 39766, 39767, 39768, 39769, 39770, 39771, 39772, 39774, 39776, 39777, 39778, 39779, 39780, 39781, 39782, 39783, 39784, 39785, 39786, 39787, 39788, 39789, 39790, 39791, 39792, 39793, 39794, 39795, 39796, 39797, 39798, 39801, 39802, 39803, 39804, 39806, 39807, 39808, 39809, 39810, 39811, 39812, 39814, 39815, 39816, 39817, 39819, 39820, 39821, 39822, 39823, 39824, 39825, 39827, 39828, 39830, 39831, 39832, 39834, 39835, 39836, 39837, 39838, 39839, 39840, 39841, 39842, 39843, 39844, 39845, 39846, 39847, 39848, 39849, 39850, 39851, 39852, 39853, 39854, 39855, 39857, 39858, 39860, 39861, 39862, 39863, 39864, 39865, 39866, 39867, 39868, 39869, 39870, 39871, 39872, 39873, 39874, 39875, 39876, 39877, 39878, 39879, 39880, 39881, 39882, 39883, 39884, 39885, 39886, 39887, 39890, 39891, 39892, 39893, 39894, 39895, 39896, 39897, 39898, 39899, 39900, 39901, 39902, 39903, 39904, 39905, 39906, 39907, 39908, 39909, 39910, 39911, 39912, 39913, 39914, 39915, 39916, 39917, 39918, 39919, 39920, 39921, 39922, 39923, 39924, 39925, 39926, 39927, 39928, 39929, 39930, 39931, 39932, 39934, 39935, 39936, 39937, 39938, 39939, 39940, 39941, 39943, 39944, 39945, 39946, 39947, 39948, 39949, 39951, 39952, 39953, 39954, 39955, 39956, 39957, 39958, 39959, 39962, 39963, 39964, 39965, 39966, 39967, 39968, 39969, 39970, 39971, 39972, 39973, 39974, 39975, 39976, 39977, 39978, 39979, 39980, 39981, 39982, 39984, 39985, 39986, 39987, 39990, 39991, 39992, 39994, 39995, 39996, 39997, 39998, 39999, 40000, 40001, 40002, 40003, 40004, 40005, 40006, 40007, 40008, 40009, 40010, 40011, 40012, 40013, 40014, 40015, 40016, 40017, 40018, 40019, 40020, 40021, 40022, 40023, 40024, 40025, 40026, 40027, 40028, 40029, 40030, 40031, 40032, 40033, 40034, 40035, 40036, 40037, 40038, 40039, 40040, 40041, 40042, 40043, 40044, 40045, 40046, 40047, 40048, 40049, 40050, 40051, 40052, 40053, 40054, 40055, 40056, 40057, 40058, 40059, 40060, 40061, 40062, 40063, 40064, 40065, 40066, 40067, 40068, 40069, 40070, 40071, 40072, 40073, 40074, 40075, 40076, 40077, 40078, 40079, 40080, 40081, 40082, 40083, 40084, 40085, 40086, 40087, 40088, 40089, 40091, 40092, 40093, 40094, 40095, 40096, 40097, 40098, 40099, 40100, 40101, 40102, 40103, 40104, 40105, 40106, 40107, 40108, 40109, 40110, 40111, 40113, 40114, 40115, 40117, 40118, 40119, 40121, 40122, 40123, 40124, 40125, 40126, 40127, 40128, 40129, 40130, 40131, 40132, 40133, 40134, 40136, 40138, 40139, 40140, 40141, 40142, 40143, 40144, 40145, 40147, 40148, 40149, 40150, 40151, 40152, 40153, 40155, 40156, 40157, 40158, 40159, 40160, 40161, 40162, 40163, 40164, 40165, 40166, 40167, 40169, 40170, 40172, 40174, 40175, 40176, 40178, 40179, 40180, 40181, 40183, 40184, 40185, 40186, 40187, 40189, 40190, 40191, 40192, 40193, 40194, 40195, 40196, 40197, 40199, 40200, 40201, 40202, 40203, 40204, 40205, 40206, 40207, 40208, 40209, 40210, 40211, 40212, 40215, 40217, 40218, 40219, 40220, 40221, 40222, 40223, 40224, 40225, 40226, 40227, 40228, 40229, 40230, 40231, 40232, 40233, 40234, 40235, 40236, 40237, 40238, 40239, 40240, 40241, 40242, 40243, 40244, 40245, 40247, 40248, 40249, 40250, 40251, 40252, 40253, 40254, 40255, 40256, 40257, 40258, 40259, 40260, 40261, 40262, 40265, 40266, 40267, 40268, 40269, 40270, 40271, 40272, 40273, 40274, 40275, 40276, 40277, 40278, 40279, 40280, 40281, 40282, 40283, 40284, 40286, 40287, 40288, 40289, 40290, 40291, 40292, 40293, 40294, 40296, 40297, 40298, 40299, 40300, 40301, 40302, 40303, 40304, 40305, 40306, 40307, 40308, 40309, 40311, 40312, 40313, 40314, 40315, 40316, 40317, 40318, 40319, 40320, 40321, 40322, 40323, 40324, 40325, 40327, 40328, 40329, 40330, 40331, 40332, 40333, 40334, 40336, 40337, 40338, 40339, 40340, 40341, 40342, 40343, 40344, 40346, 40347, 40349, 40350, 40351, 40352, 40353, 40354, 40356, 40357, 40358, 40360, 40361, 40362, 40363, 40364, 40365, 40366, 40367, 40368, 40369, 40370, 40371, 40372, 40373, 40375, 40376, 40377, 40378, 40379, 40380, 40381, 40383, 40384, 40385, 40386, 40387, 40388, 40389, 40390, 40391, 40392, 40393, 40394, 40395, 40396, 40397, 40398, 40399, 40400, 40401, 40402, 40403, 40404, 40405, 40406, 40407, 40408, 40409, 40411, 40413, 40414, 40415, 40416, 40417, 40418, 40419, 40420, 40421, 40422, 40423, 40424, 40425, 40427, 40428, 40429, 40432, 40433, 40434, 40435, 40436, 40437, 40438, 40439, 40440, 40441, 40442, 40445, 40446, 40447, 40448, 40449, 40450, 40451, 40452, 40453, 40454, 40455, 40458, 40460, 40461, 40462, 40464, 40465, 40466, 40467, 40468, 40469, 40471, 40472, 40473, 40474, 40475, 40476, 40477, 40478, 40479, 40480, 40481, 40482, 40483, 40484, 40485, 40486, 40487, 40488, 40489, 40490, 40492, 40493, 40494, 40495, 40497, 40498, 40499, 40500, 40501, 40502, 40503, 40504, 40505, 40506, 40507, 40508, 40509, 40511, 40512, 40513, 40514, 40515, 40516, 40517, 40519, 40520, 40521, 40522, 40525, 40527, 40528, 40529, 40530, 40531, 40532, 40533, 40535, 40536, 40537, 40538, 40539, 40540, 40541, 40542, 40543, 40544, 40545, 40546, 40547, 40548, 40549, 40550, 40551, 40552, 40553, 40554, 40555, 40556, 40557, 40559, 40560, 40561, 40562, 40563, 40564, 40565, 40566, 40567, 40568, 40569, 40570, 40571, 40574, 40575, 40576, 40578, 40579, 40580, 40581, 40582, 40583, 40585, 40586, 40587, 40588, 40589, 40590, 40591, 40592, 40593, 40594, 40595, 40596, 40597, 40598, 40599, 40600, 40601, 40602, 40603, 40604, 40605, 40606, 40607, 40608, 40609, 40610, 40611, 40612, 40613, 40614, 40615, 40616, 40617, 40618, 40620, 40621, 40622, 40623, 40624, 40625, 40626, 40627, 40628, 40629, 40630, 40631, 40632, 40633, 40634, 40635, 40636, 40637, 40638, 40639, 40640, 40641, 40642, 40643, 40644, 40645, 40646, 40647, 40648, 40649, 40650, 40651, 40652, 40653, 40654, 40655, 40656, 40657, 40658, 40659, 40660, 40661, 40662, 40663, 40664, 40665, 40666, 40668, 40669, 40670, 40671, 40672, 40673, 40674, 40675, 40676, 40677, 40678, 40679, 40680, 40682, 40683, 40684, 40685, 40686, 40688, 40689, 40690, 40691, 40692, 40693, 40694, 40695, 40696, 40697, 40698, 40699, 40700, 40701, 40702, 40703, 40704, 40706, 40707, 40708, 40709, 40710, 40711, 40712, 40713, 40714, 40715, 40716, 40717, 40718, 40719, 40721, 40722, 40723, 40724, 40725, 40726, 40727, 40728, 40729, 40730, 40731, 40732, 40733, 40734, 40735, 40736, 40737, 40738, 40739, 40740, 40741, 40742, 40743, 40744, 40745, 40748, 40749, 40750, 40751, 40752, 40753, 40754, 40755, 40756, 40758, 40759, 40760, 40761, 40762, 40763, 40764, 40765, 40766, 40767, 40768, 40770, 40771, 40774, 40775, 40776, 40777, 40778, 40779, 40781, 40782, 40783, 40784, 40785, 40786, 40787, 40788, 40789, 40790, 40791, 40792, 40793, 40794, 40795, 40796, 40797, 40798, 40799, 40800, 40801, 40802, 40803, 40804, 40805, 40806, 40807, 40808, 40810, 40811, 40812, 40813, 40814, 40815, 40816, 40817, 40818, 40819, 40821, 40822, 40823, 40824, 40825, 40826, 40827, 40828, 40829, 40830, 40831, 40832, 40833, 40834, 40835, 40836, 40838, 40839, 40840, 40841, 40842, 40843, 40844, 40845, 40846, 40847, 40849, 40850, 40851, 40852, 40853, 40854, 40855, 40856, 40857, 40858, 40859, 40860, 40861, 40862, 40863, 40865, 40866, 40867, 40868, 40869, 40870, 40871, 40872, 40873, 40874, 40876, 40878, 40879, 40880, 40881, 40883, 40884, 40885, 40886, 40887, 40888, 40889, 40890, 40891, 40892, 40893, 40894, 40895, 40896, 40897, 40898, 40899, 40900, 40901, 40902, 40903, 40904, 40905, 40906, 40907, 40908, 40909, 40910, 40911, 40912, 40913, 40914, 40915, 40916, 40917, 40919, 40920, 40921, 40922, 40923, 40924, 40925, 40926, 40927, 40928, 40929, 40930, 40933, 40934, 40935, 40936, 40937, 40938, 40939, 40940, 40941, 40942, 40943, 40944, 40945, 40946, 40948, 40949, 40950, 40951, 40952, 40953, 40954, 40955, 40956, 40957, 40958, 40959, 40960, 40961, 40962, 40963, 40964, 40966, 40967, 40968, 40969, 40970, 40971, 40972, 40973, 40974, 40975, 40976, 40977, 40978, 40979, 40980, 40981, 40982, 40983, 40984, 40985, 40986, 40987, 40988, 40989, 40990, 40991, 40993, 40994, 40995, 40996, 40997, 40998, 40999, 41000, 41001, 41002, 41003, 41004, 41005, 41006, 41008, 41009, 41011, 41012, 41013, 41014, 41015, 41016, 41017, 41018, 41019, 41020, 41021, 41023, 41024, 41025, 41026, 41027, 41028, 41029, 41030, 41031, 41032, 41033, 41034, 41035, 41036, 41037, 41038, 41039, 41040, 41041, 41042, 41043, 41044, 41045, 41046, 41047, 41048, 41049, 41050, 41051, 41052, 41053, 41054, 41055, 41056, 41057, 41058, 41059, 41060, 41061, 41062, 41063, 41064, 41066, 41067, 41068, 41069, 41070, 41071, 41072, 41073, 41074, 41076, 41077, 41078, 41079, 41081, 41083, 41084, 41085, 41086, 41087, 41088, 41090, 41091, 41092, 41093, 41095, 41096, 41097, 41098, 41099, 41100, 41101, 41102, 41103, 41104, 41105, 41106, 41108, 41109, 41111, 41112, 41113, 41115, 41117, 41118, 41119, 41120, 41121, 41122, 41123, 41125, 41126, 41127, 41128, 41130, 41131, 41132, 41133, 41134, 41135, 41136, 41137, 41138, 41139, 41140, 41141, 41142, 41143, 41144, 41145, 41146, 41147, 41148, 41150, 41151, 41152, 41153, 41154, 41155, 41156, 41157, 41158, 41159, 41160, 41161, 41162, 41163, 41164, 41165, 41169, 41170, 41171, 41172, 41173, 41174, 41175, 41176, 41178, 41180, 41181, 41182, 41183, 41184, 41185, 41186, 41187, 41188, 41189, 41190, 41191, 41192, 41193, 41194, 41196, 41197, 41198, 41199, 41200, 41201, 41202, 41203, 41204, 41205, 41206, 41208, 41210, 41211, 41213, 41214, 41215, 41216, 41217, 41218, 41219, 41220, 41221, 41222, 41223, 41224, 41225, 41227, 41228, 41229, 41230, 41231, 41232, 41233, 41234, 41235, 41236, 41237, 41238, 41239, 41240, 41242, 41244, 41245, 41246, 41247, 41248, 41249, 41250, 41251, 41252, 41253, 41254, 41255, 41256, 41257, 41258, 41259, 41260, 41261, 41262, 41263, 41264, 41265, 41266, 41267, 41268, 41269, 41270, 41271, 41272, 41273, 41274, 41275, 41276, 41277, 41279, 41280, 41281, 41282, 41283, 41284, 41285, 41286, 41287, 41288, 41289, 41291, 41292, 41293, 41294, 41295, 41296, 41297, 41298, 41299, 41300, 41302, 41303, 41304, 41305, 41306, 41307, 41308, 41309, 41310, 41311, 41312, 41313, 41314, 41315, 41316, 41317, 41318, 41319, 41320, 41321, 41322, 41323, 41324, 41325, 41326, 41327, 41328, 41329, 41330, 41331, 41332, 41333, 41335, 41336, 41337, 41338, 41339, 41340, 41341, 41342, 41343, 41344, 41345, 41346, 41347, 41348, 41350, 41351, 41352, 41353, 41354, 41355, 41356, 41357, 41358, 41359, 41360, 41361, 41362, 41363, 41364, 41365, 41366, 41367, 41368, 41369, 41371, 41372, 41373, 41374, 41376, 41377, 41378, 41379, 41380, 41381, 41382, 41383, 41384, 41385, 41386, 41387, 41388, 41389, 41390, 41391, 41392, 41393, 41394, 41395, 41396, 41397, 41398, 41399, 41400, 41401, 41403, 41404, 41405, 41407, 41408, 41409, 41410, 41411, 41412, 41413, 41414, 41415, 41416, 41417, 41418, 41420, 41421, 41422, 41423, 41424, 41425, 41426, 41427, 41428, 41429, 41430, 41431, 41432, 41434, 41437, 41438, 41439, 41440, 41441, 41442, 41443, 41444, 41445, 41446, 41447, 41448, 41449, 41450, 41451, 41452, 41453, 41454, 41455, 41456, 41457, 41458, 41460, 41461, 41462, 41463, 41464, 41465, 41466, 41467, 41469, 41470, 41471, 41472, 41473, 41474, 41475, 41476, 41477, 41478, 41479, 41480, 41481, 41482, 41483, 41484, 41485, 41487, 41488, 41489, 41490, 41491, 41492, 41493, 41494, 41496, 41497, 41498, 41499, 41500, 41501, 41502, 41503, 41504, 41505, 41506, 41507, 41508, 41509, 41510, 41511, 41512, 41513, 41514, 41515, 41516, 41517, 41518, 41519, 41521, 41522, 41523, 41524, 41525, 41526, 41527, 41528, 41529, 41532, 41534, 41535, 41536, 41537, 41538, 41539, 41540, 41541, 41542, 41543, 41544, 41545, 41546, 41547, 41549, 41550, 41551, 41552, 41553, 41554, 41555, 41556, 41557, 41558, 41559, 41560, 41562, 41564, 41565, 41566, 41567, 41568, 41569, 41570, 41572, 41573, 41574, 41575, 41576, 41577, 41578, 41579, 41580, 41581, 41582, 41583, 41584, 41585, 41587, 41588, 41589, 41590, 41591, 41592, 41593, 41595, 41596, 41597, 41598, 41599, 41600, 41601, 41602, 41603, 41604, 41605, 41606, 41607, 41608, 41609, 41610, 41611, 41612, 41613, 41614, 41615, 41616, 41619, 41620, 41621, 41622, 41623, 41624, 41625, 41626, 41627, 41628, 41629, 41630, 41631, 41632, 41633, 41634, 41635, 41636, 41637, 41638, 41639, 41640, 41643, 41644, 41645, 41646, 41647, 41648, 41649, 41650, 41651, 41652, 41653, 41654, 41655, 41656, 41657, 41658, 41659, 41660, 41661, 41662, 41663, 41664, 41665, 41666, 41667, 41668, 41669, 41670, 41671, 41673, 41674, 41675, 41676, 41677, 41678, 41680, 41681, 41682, 41683, 41684, 41685, 41686, 41687, 41688, 41689, 41690, 41691, 41692, 41694, 41695, 41696, 41697, 41698, 41699, 41700, 41701, 41702, 41704, 41705, 41707, 41708, 41710, 41711, 41712, 41713, 41714, 41715, 41716, 41717, 41719, 41720, 41721, 41722, 41723, 41724, 41725, 41727, 41728, 41729, 41730, 41731, 41733, 41734, 41735, 41736, 41737, 41738, 41739, 41740, 41741, 41742, 41743, 41744, 41745, 41746, 41748, 41749, 41750, 41751, 41752, 41753, 41754, 41756, 41757, 41758, 41759, 41760, 41761, 41762, 41763, 41765, 41766, 41767, 41768, 41769, 41770, 41771, 41772, 41773, 41774, 41775, 41776, 41778, 41779, 41780, 41781, 41782, 41783, 41784, 41785, 41786, 41787, 41788, 41789, 41790, 41791, 41792, 41793, 41794, 41795, 41796, 41797, 41798, 41799, 41800, 41801, 41802, 41803, 41804, 41805, 41806, 41807, 41808, 41809, 41810, 41811, 41812, 41813, 41814, 41815, 41816, 41817, 41818, 41819, 41820, 41821, 41822, 41823, 41824, 41825, 41828, 41829, 41830, 41831, 41832, 41833, 41834, 41835, 41836, 41837, 41838, 41839, 41840, 41841, 41842, 41843, 41845, 41846, 41847, 41848, 41849, 41850, 41851, 41852, 41853, 41854, 41856, 41857, 41858, 41859, 41860, 41861, 41862, 41863, 41864, 41865, 41866, 41867, 41868, 41869, 41870, 41871, 41872, 41873, 41874, 41876, 41877, 41879, 41880, 41881, 41882, 41883, 41884, 41885, 41886, 41887, 41889, 41890, 41891, 41892, 41893, 41894, 41895, 41896, 41897, 41898, 41899, 41900, 41901, 41902, 41904, 41905, 41906, 41907, 41908, 41909, 41910, 41911, 41912, 41913, 41914, 41915, 41916, 41917, 41918, 41919, 41921, 41922, 41923, 41924, 41925, 41926, 41927, 41928, 41930, 41931, 41932, 41934, 41935, 41936, 41937, 41938, 41939, 41940, 41941, 41942, 41943, 41944, 41945, 41946, 41947, 41948, 41949, 41950, 41951, 41952, 41953, 41954, 41955, 41956, 41957, 41958, 41959, 41960, 41961, 41962, 41963, 41964, 41965, 41966, 41967, 41968, 41969, 41970, 41971, 41972, 41974, 41975, 41976, 41977, 41978, 41979, 41980, 41981, 41982, 41983, 41984, 41985, 41986, 41987, 41988, 41990, 41991, 41992, 41993, 41994, 41995, 41996, 41997, 41998, 41999]\n"
     ]
    }
   ],
   "source": [
    "count = np.zeros(10).astype('int64')\n",
    "index = []\n",
    "for i in range(len(y_train)):\n",
    "    if count[y_train[i]] < min_val:\n",
    "        count[y_train[i]] += 1\n",
    "    else:\n",
    "        index.append(i)\n",
    "\n",
    "print(index)\n",
    "        \n",
    "x_train = np.delete(x_train, index,0)\n",
    "y_train = np.delete(y_train, index,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, num_classes = 10)\n",
    "\n",
    "x_train = x_train.astype('float32')/255\n",
    "x_train = x_train.reshape([-1,28,28,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu', input_shape = (28,28,1)))\n",
    "model.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters = 256, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(Conv2D(filters = 256, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(filters = 512, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(Conv2D(filters = 512, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(Conv2D(filters = 512, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 128)       1280      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 7, 7, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 7, 7, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              4719616   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 11,663,754\n",
      "Trainable params: 11,663,754\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = Adam() , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)\n",
    "\n",
    "filepath=\"weights-improvement-{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "callbacks_list = [checkpoint, learning_rate_reduction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34155 samples, validate on 3795 samples\n",
      "Epoch 1/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9804\n",
      "Epoch 00001: val_acc improved from -inf to 0.98524, saving model to weights-improvement-01-0.9852.hdf5\n",
      "34155/34155 [==============================] - 15s 434us/step - loss: 0.0661 - acc: 0.9805 - val_loss: 0.0537 - val_acc: 0.9852\n",
      "Epoch 2/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9849\n",
      "Epoch 00002: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 406us/step - loss: 0.0507 - acc: 0.9849 - val_loss: 0.0812 - val_acc: 0.9794\n",
      "Epoch 3/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9882\n",
      "Epoch 00003: val_acc improved from 0.98524 to 0.98735, saving model to weights-improvement-03-0.9874.hdf5\n",
      "34155/34155 [==============================] - 14s 423us/step - loss: 0.0407 - acc: 0.9883 - val_loss: 0.0461 - val_acc: 0.9874\n",
      "Epoch 4/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9898\n",
      "Epoch 00004: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 409us/step - loss: 0.0328 - acc: 0.9898 - val_loss: 0.0612 - val_acc: 0.9839\n",
      "Epoch 5/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9914\n",
      "Epoch 00005: val_acc improved from 0.98735 to 0.98841, saving model to weights-improvement-05-0.9884.hdf5\n",
      "34155/34155 [==============================] - 14s 423us/step - loss: 0.0293 - acc: 0.9914 - val_loss: 0.0442 - val_acc: 0.9884\n",
      "Epoch 6/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9916\n",
      "Epoch 00006: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 409us/step - loss: 0.0284 - acc: 0.9916 - val_loss: 0.0409 - val_acc: 0.9881\n",
      "Epoch 7/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9927\n",
      "Epoch 00007: val_acc improved from 0.98841 to 0.98946, saving model to weights-improvement-07-0.9895.hdf5\n",
      "34155/34155 [==============================] - 14s 423us/step - loss: 0.0247 - acc: 0.9927 - val_loss: 0.0435 - val_acc: 0.9895\n",
      "Epoch 8/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9938\n",
      "Epoch 00008: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 409us/step - loss: 0.0206 - acc: 0.9938 - val_loss: 0.0482 - val_acc: 0.9879\n",
      "Epoch 9/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9925\n",
      "Epoch 00009: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 410us/step - loss: 0.0248 - acc: 0.9925 - val_loss: 0.0479 - val_acc: 0.9879\n",
      "Epoch 10/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9946\n",
      "Epoch 00010: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 410us/step - loss: 0.0175 - acc: 0.9946 - val_loss: 0.0561 - val_acc: 0.9871\n",
      "Epoch 11/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9931\n",
      "Epoch 00011: val_acc did not improve\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "34155/34155 [==============================] - 14s 412us/step - loss: 0.0235 - acc: 0.9931 - val_loss: 0.0731 - val_acc: 0.9852\n",
      "Epoch 12/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9973\n",
      "Epoch 00012: val_acc improved from 0.98946 to 0.99209, saving model to weights-improvement-12-0.9921.hdf5\n",
      "34155/34155 [==============================] - 15s 425us/step - loss: 0.0091 - acc: 0.9973 - val_loss: 0.0467 - val_acc: 0.9921\n",
      "Epoch 13/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9983\n",
      "Epoch 00013: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 410us/step - loss: 0.0058 - acc: 0.9983 - val_loss: 0.0435 - val_acc: 0.9895\n",
      "Epoch 14/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9987\n",
      "Epoch 00014: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 410us/step - loss: 0.0040 - acc: 0.9987 - val_loss: 0.0579 - val_acc: 0.9903\n",
      "Epoch 15/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9990\n",
      "Epoch 00015: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 410us/step - loss: 0.0033 - acc: 0.9990 - val_loss: 0.0480 - val_acc: 0.9908\n",
      "Epoch 16/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9988\n",
      "Epoch 00016: val_acc improved from 0.99209 to 0.99289, saving model to weights-improvement-16-0.9929.hdf5\n",
      "34155/34155 [==============================] - 15s 426us/step - loss: 0.0046 - acc: 0.9988 - val_loss: 0.0425 - val_acc: 0.9929\n",
      "Epoch 17/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9988\n",
      "Epoch 00017: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 410us/step - loss: 0.0043 - acc: 0.9988 - val_loss: 0.0488 - val_acc: 0.9926\n",
      "Epoch 18/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9985\n",
      "Epoch 00018: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 411us/step - loss: 0.0053 - acc: 0.9985 - val_loss: 0.0644 - val_acc: 0.9900\n",
      "Epoch 19/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9977\n",
      "Epoch 00019: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 411us/step - loss: 0.0076 - acc: 0.9977 - val_loss: 0.0491 - val_acc: 0.9913\n",
      "Epoch 20/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9983\n",
      "Epoch 00020: val_acc did not improve\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "34155/34155 [==============================] - 14s 410us/step - loss: 0.0058 - acc: 0.9983 - val_loss: 0.0480 - val_acc: 0.9924\n",
      "Epoch 21/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9988\n",
      "Epoch 00021: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 410us/step - loss: 0.0043 - acc: 0.9988 - val_loss: 0.0431 - val_acc: 0.9916\n",
      "Epoch 22/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9994\n",
      "Epoch 00022: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 410us/step - loss: 0.0018 - acc: 0.9994 - val_loss: 0.0469 - val_acc: 0.9918\n",
      "Epoch 23/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 00023: val_acc did not improve\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "34155/34155 [==============================] - 14s 410us/step - loss: 0.0015 - acc: 0.9995 - val_loss: 0.0462 - val_acc: 0.9916\n",
      "Epoch 24/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9997\n",
      "Epoch 00024: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 409us/step - loss: 0.0013 - acc: 0.9997 - val_loss: 0.0531 - val_acc: 0.9916\n",
      "Epoch 25/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 5.3373e-04 - acc: 0.9998\n",
      "Epoch 00025: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 410us/step - loss: 5.3233e-04 - acc: 0.9998 - val_loss: 0.0479 - val_acc: 0.9921\n",
      "Epoch 26/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 9.4403e-04 - acc: 0.9997\n",
      "Epoch 00026: val_acc did not improve\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "34155/34155 [==============================] - 14s 410us/step - loss: 9.4108e-04 - acc: 0.9997 - val_loss: 0.0478 - val_acc: 0.9918\n",
      "Epoch 27/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 7.6469e-04 - acc: 0.9998\n",
      "Epoch 00027: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 411us/step - loss: 7.6232e-04 - acc: 0.9998 - val_loss: 0.0496 - val_acc: 0.9921\n",
      "Epoch 28/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 5.0689e-04 - acc: 0.9999\n",
      "Epoch 00028: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 411us/step - loss: 5.0564e-04 - acc: 0.9999 - val_loss: 0.0510 - val_acc: 0.9913\n",
      "Epoch 29/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 3.1107e-04 - acc: 0.9999\n",
      "Epoch 00029: val_acc did not improve\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "34155/34155 [==============================] - 14s 410us/step - loss: 3.1018e-04 - acc: 0.9999 - val_loss: 0.0505 - val_acc: 0.9916\n",
      "Epoch 30/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 2.1821e-04 - acc: 1.0000\n",
      "Epoch 00030: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 410us/step - loss: 2.1756e-04 - acc: 1.0000 - val_loss: 0.0507 - val_acc: 0.9916\n",
      "Epoch 31/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 2.2781e-04 - acc: 0.9999- ETA: 1s - loss\n",
      "Epoch 00031: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 408us/step - loss: 2.2715e-04 - acc: 0.9999 - val_loss: 0.0512 - val_acc: 0.9918\n",
      "Epoch 32/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 2.0491e-04 - acc: 0.9999\n",
      "Epoch 00032: val_acc did not improve\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "34155/34155 [==============================] - 14s 408us/step - loss: 2.0476e-04 - acc: 0.9999 - val_loss: 0.0511 - val_acc: 0.9924\n",
      "Epoch 33/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 2.6367e-04 - acc: 0.9999\n",
      "Epoch 00033: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 408us/step - loss: 2.6285e-04 - acc: 0.9999 - val_loss: 0.0517 - val_acc: 0.9921\n",
      "Epoch 34/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 1.3813e-04 - acc: 0.9999\n",
      "Epoch 00034: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 408us/step - loss: 1.3773e-04 - acc: 0.9999 - val_loss: 0.0520 - val_acc: 0.9921\n",
      "Epoch 35/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 1.6062e-04 - acc: 0.9999\n",
      "Epoch 00035: val_acc did not improve\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "34155/34155 [==============================] - 14s 408us/step - loss: 1.6013e-04 - acc: 0.9999 - val_loss: 0.0519 - val_acc: 0.9924\n",
      "Epoch 36/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 2.8828e-04 - acc: 0.9999\n",
      "Epoch 00036: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 408us/step - loss: 2.8740e-04 - acc: 0.9999 - val_loss: 0.0516 - val_acc: 0.9924\n",
      "Epoch 37/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 1.0980e-04 - acc: 1.0000\n",
      "Epoch 00037: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 408us/step - loss: 1.0947e-04 - acc: 1.0000 - val_loss: 0.0516 - val_acc: 0.9924\n",
      "Epoch 38/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 2.2948e-04 - acc: 0.9999\n",
      "Epoch 00038: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 408us/step - loss: 2.2941e-04 - acc: 0.9999 - val_loss: 0.0509 - val_acc: 0.9926\n",
      "Epoch 39/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 1.6359e-04 - acc: 0.9999\n",
      "Epoch 00039: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 408us/step - loss: 1.6351e-04 - acc: 0.9999 - val_loss: 0.0510 - val_acc: 0.9926\n",
      "Epoch 40/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 2.0284e-04 - acc: 0.9999\n",
      "Epoch 00040: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 408us/step - loss: 2.0225e-04 - acc: 0.9999 - val_loss: 0.0502 - val_acc: 0.9924\n",
      "Epoch 41/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 2.5568e-04 - acc: 0.9999\n",
      "Epoch 00041: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 410us/step - loss: 2.5987e-04 - acc: 0.9999 - val_loss: 0.0503 - val_acc: 0.9926\n",
      "Epoch 42/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 1.5778e-04 - acc: 0.9999\n",
      "Epoch 00042: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 409us/step - loss: 1.5750e-04 - acc: 0.9999 - val_loss: 0.0503 - val_acc: 0.9926\n",
      "Epoch 43/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 1.1169e-04 - acc: 1.0000\n",
      "Epoch 00043: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 408us/step - loss: 1.1144e-04 - acc: 1.0000 - val_loss: 0.0510 - val_acc: 0.9926\n",
      "Epoch 44/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 1.2308e-04 - acc: 0.9999\n",
      "Epoch 00044: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 408us/step - loss: 1.2276e-04 - acc: 0.9999 - val_loss: 0.0508 - val_acc: 0.9929\n",
      "Epoch 45/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 1.7321e-04 - acc: 0.9999\n",
      "Epoch 00045: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 408us/step - loss: 1.7271e-04 - acc: 0.9999 - val_loss: 0.0509 - val_acc: 0.9929\n",
      "Epoch 46/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 8.3179e-05 - acc: 1.0000\n",
      "Epoch 00046: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 408us/step - loss: 8.2957e-05 - acc: 1.0000 - val_loss: 0.0510 - val_acc: 0.9929\n",
      "Epoch 47/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 7.7200e-05 - acc: 1.0000\n",
      "Epoch 00047: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 408us/step - loss: 7.7040e-05 - acc: 1.0000 - val_loss: 0.0521 - val_acc: 0.9929\n",
      "Epoch 48/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 1.1444e-04 - acc: 1.0000\n",
      "Epoch 00048: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 409us/step - loss: 1.1434e-04 - acc: 1.0000 - val_loss: 0.0529 - val_acc: 0.9926\n",
      "Epoch 49/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 6.4463e-05 - acc: 1.0000\n",
      "Epoch 00049: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 408us/step - loss: 6.4262e-05 - acc: 1.0000 - val_loss: 0.0529 - val_acc: 0.9929\n",
      "Epoch 50/50\n",
      "34048/34155 [============================>.] - ETA: 0s - loss: 7.8030e-05 - acc: 1.0000\n",
      "Epoch 00050: val_acc did not improve\n",
      "34155/34155 [==============================] - 14s 409us/step - loss: 7.7790e-05 - acc: 1.0000 - val_loss: 0.0539 - val_acc: 0.9929\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,y_train, batch_size=batch_size,\n",
    "                              epochs = epochs,\n",
    "                              verbose = 1, validation_split=0.1,\n",
    "                              callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert model into JSON Format\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "model.save_weights(\"finished_{}.hdf5\".format(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
